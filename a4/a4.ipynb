{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cfce32-4eaf-4a48-bb47-3becf0cbd148",
   "metadata": {},
   "source": [
    "# Assignment 4 - CS131\n",
    "### Shashhank Seethula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385bc8bb-4127-48e8-b228-6595a13aa63f",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8d229a1-76ff-4519-bd13-27a5102f2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d8b537-1bf8-4bc9-80d0-484f68ec988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------+------------+\n",
      "|passenger_count|pulocationid|dolocationid|total_amount|\n",
      "+---------------+------------+------------+------------+\n",
      "|            1.0|       239.0|       239.0|         8.8|\n",
      "|            1.0|       230.0|       100.0|         8.3|\n",
      "|            1.0|        68.0|       127.0|       47.75|\n",
      "|            1.0|        68.0|        68.0|         7.3|\n",
      "|            1.0|        50.0|        42.0|       23.15|\n",
      "|            1.0|        95.0|       196.0|         9.8|\n",
      "|            1.0|       211.0|       211.0|         6.8|\n",
      "|            1.0|       237.0|       162.0|         7.8|\n",
      "|            1.0|       148.0|        37.0|        20.3|\n",
      "|            1.0|       265.0|       265.0|        0.31|\n",
      "+---------------+------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the csv file stored in my apache_spark bucket\n",
    "df = spark.read.csv(\"gs://apache_spark/2019-04.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Only select needed columns (passenger_count, pulocationid, dolocationid, total_amount)\n",
    "df_new = df.select(\"passenger_count\", \"pulocationid\", \"dolocationid\", \"total_amount\")\n",
    "\n",
    "# Display the first 10 rows\n",
    "df_new.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8205029-ea1a-436b-87cd-6bf29d48c9b1",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "508038e7-c01c-4ed1-9073-bc71f073cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an 80/20 split\n",
    "trainDF, testDF = df_new.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a760f-6da0-483e-bbee-389d56237254",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed18c943-fca2-46d3-b50d-37ec6664854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------+---------------+------------+\n",
      "|passenger_count|pulocationid|dolocationid|       features|total_amount|\n",
      "+---------------+------------+------------+---------------+------------+\n",
      "|            0.0|         1.0|         1.0|  [0.0,1.0,1.0]|         0.3|\n",
      "|            0.0|         1.0|         1.0|  [0.0,1.0,1.0]|        16.8|\n",
      "|            0.0|         1.0|         1.0|  [0.0,1.0,1.0]|       114.3|\n",
      "|            0.0|         1.0|       264.0|[0.0,1.0,264.0]|      119.75|\n",
      "|            0.0|         3.0|       136.0|[0.0,3.0,136.0]|        22.8|\n",
      "|            0.0|         4.0|         4.0|  [0.0,4.0,4.0]|        60.3|\n",
      "|            0.0|         4.0|        49.0| [0.0,4.0,49.0]|        26.0|\n",
      "|            0.0|         4.0|        68.0| [0.0,4.0,68.0]|       18.95|\n",
      "|            0.0|         4.0|        75.0| [0.0,4.0,75.0]|       28.55|\n",
      "|            0.0|         4.0|        79.0| [0.0,4.0,79.0]|         7.3|\n",
      "+---------------+------------+------------+---------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Use VectorAssembler to create the features column\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(\n",
    "    inputCols=[\"passenger_count\", \"pulocationid\", \"dolocationid\"],\n",
    "    outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select(\"passenger_count\", \"pulocationid\", \"dolocationid\", \"features\", \"total_amount\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "170b382a-75bb-4395-a889-2275911f0163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Use DecisionTreeRegressor to predict total_amount\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"total_amount\")\n",
    "dtModel = dt.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150138ba-00c2-495f-91bc-916c2d9d11ef",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6fc3d39-f527-45c8-adfd-7b0dc90ebc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with VectorAseember and DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[vecAssembler, dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d93e69-c72e-44b5-91a4-c2e7d753de21",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95691c27-f950-4f23-8287-2b73a3c8d841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c945a7a-6461-4509-a93b-2448ba0113f6",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fd436f0-61ff-431d-8097-f7eea9d7827e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------+---------------+------------+------------------+\n",
      "|passenger_count|pulocationid|dolocationid|       features|total_amount|        prediction|\n",
      "+---------------+------------+------------+---------------+------------+------------------+\n",
      "|            0.0|         1.0|         1.0|  [0.0,1.0,1.0]|       103.3|19.711640780100204|\n",
      "|            0.0|         4.0|         4.0|  [0.0,4.0,4.0]|         6.8|19.711640780100204|\n",
      "|            0.0|         4.0|        33.0| [0.0,4.0,33.0]|       31.55|15.648492562197445|\n",
      "|            0.0|         4.0|        79.0| [0.0,4.0,79.0]|         7.8|15.648492562197445|\n",
      "|            0.0|         4.0|       107.0|[0.0,4.0,107.0]|        11.8|15.648492562197445|\n",
      "|            0.0|         4.0|       144.0|[0.0,4.0,144.0]|        11.3|17.806537849630722|\n",
      "|            0.0|         4.0|       234.0|[0.0,4.0,234.0]|        11.0|17.806537849630722|\n",
      "|            0.0|         7.0|       121.0|[0.0,7.0,121.0]|        28.8|17.806537849630722|\n",
      "|            0.0|         7.0|       223.0|[0.0,7.0,223.0]|         6.8|17.806537849630722|\n",
      "|            0.0|         7.0|       223.0|[0.0,7.0,223.0]|         8.3|17.806537849630722|\n",
      "+---------------+------------+------------+---------------+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Retrieve predictions from model and show the top 10 rows\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"passenger_count\", \"pulocationid\", \n",
    "              \"dolocationid\", \"features\",\n",
    "              \"total_amount\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3583470-ee60-42b2-8689-1815225aa0df",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96a857a9-670e-49b4-b3ef-e1089ba0646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:==============>                                           (2 + 6) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 12.206149635040326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Use RegressionEvaluator and display the RMSE between prediction and total_amount\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"total_amount\",\n",
    "    metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "\n",
    "print(\"Root Mean Squared Error: \" + str(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs131-jupyter-spark on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-runtime-0000ecf6805c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
